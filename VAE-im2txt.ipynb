{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n",
      "/home/icarus/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import lasagne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from lasagne.layers import DenseLayer, DropoutLayer, NonlinearityLayer, InputLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.nonlinearities import softmax, leaky_rectify\n",
    "from lasagne.layers import get_output, get_all_params\n",
    "import theano\n",
    "\n",
    "from samplelayer import SimpleSampleLayer\n",
    "\n",
    "#theano.config.optimizer = 'None'\n",
    "theano.config.exception_verbosity='high'\n",
    "theano.config.compute_test_value = 'warn' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'x_train_100Samples.p'\n",
    "VALIDATION_FILE = 'x_train_100SamplesValidation.p'\n",
    "TARGET_FILE = 'targets_train_100Samples.p'\n",
    "TARGET_VALIDATION_FILE = 'targets_train_100SamplesValidation.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VAEHelper:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_theano_functions(net):\n",
    "        sym_x = T.tensor4('x')\n",
    "        sym_x.tag.test_value = np.zeros((10,3,224,224), dtype = np.float32)\n",
    "        sym_z = T.matrix('z')\n",
    "        sym_z.tag.test_value = np.zeros((10,2), dtype = np.float32)\n",
    "        sym_y = T.matrix('y')\n",
    "        sym_y.tag.test_value = np.zeros((10,4), dtype = np.float32)\n",
    "        \n",
    "        z_train, muq_train, logvarq_train = get_output([net['z_vae'], net['muq_vae'], net['logvarq_vae']],\n",
    "                                                       {net['input']: sym_x}, deterministic=False)\n",
    "        prob_train = get_output(net['prob'], {net['in_z_vae']: z_train}, deterministic=False)\n",
    "\n",
    "        z_eval, muq_eval, logvarq_eval = get_output([net['z_vae'], net['muq_vae'], net['logvarq_vae']],\n",
    "                                                    {net['input']: sym_x}, deterministic=True)\n",
    "        prob_eval = get_output(net['prob'], {net['in_z_vae']: z_eval}, deterministic=True)\n",
    "\n",
    "        prob_sample = get_output(net['prob'], {net['in_z_vae']: sym_z}, deterministic=True)\n",
    "\n",
    "        LL_train, logpx_train, KL_train = VAEHelper.LogLikelihood(prob_train, sym_y, muq_train, logvarq_train)\n",
    "        LL_eval, logpx_eval, KL_eval = VAEHelper.LogLikelihood(prob_eval, sym_y, muq_eval, logvarq_eval)\n",
    "\n",
    "        all_params = get_all_params([net['z_vae'],net['prob']],trainable=True)\n",
    "\n",
    "        # Let Theano do its magic and get all the gradients we need for training\n",
    "        all_grads = T.grad(-LL_train, all_params)\n",
    "\n",
    "        # Set the update function for parameters. The Adam optimizer works really well with VAEs.\n",
    "        updates = lasagne.updates.adam(all_grads, all_params, learning_rate=1e-2)\n",
    "\n",
    "        f_train = theano.function(inputs=[sym_x, sym_y],\n",
    "                                  outputs=[LL_train, logpx_train, KL_train],\n",
    "                                  updates=updates)\n",
    "\n",
    "        f_eval = theano.function(inputs=[sym_x, sym_y],\n",
    "                                 outputs=[LL_eval, logpx_eval, KL_eval])\n",
    "\n",
    "        f_z = theano.function(inputs=[sym_x],\n",
    "                              outputs=[z_eval])\n",
    "\n",
    "        f_sample = theano.function(inputs=[sym_z],\n",
    "                                   outputs=[prob_sample])\n",
    "\n",
    "        f_recon = theano.function(inputs=[sym_x],\n",
    "                                  outputs=[prob_eval])\n",
    "\n",
    "        return f_train, f_eval, f_z, f_sample, f_recon\n",
    "\n",
    "    @staticmethod\n",
    "    def LogLikelihood(mux,x,muq,logvarq):\n",
    "        log_px_given_z = VAEHelper.log_bernoulli(x, mux, eps=1e-6).sum(axis=1).mean() #note that we sum the latent dimension and mean over the samples\n",
    "        KL_qp = VAEHelper.kl_normal2_stdnormal(muq, logvarq).sum(axis=1).mean() # * 0 # To ignore the KL term\n",
    "        LL = log_px_given_z - KL_qp\n",
    "        return LL, log_px_given_z, KL_qp\n",
    "\n",
    "    @staticmethod\n",
    "    def log_bernoulli(x, p, eps=0.0):\n",
    "        p = T.clip(p, eps, 1.0 - eps)\n",
    "        return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_normal2_stdnormal(mean, log_var):\n",
    "        return -0.5*(1 + log_var - mean**2 - T.exp(log_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_LATENT_Z = 2\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "def create_network():\n",
    "    net = {}\n",
    "\n",
    "    # VGG Net\n",
    "    net['input'] = InputLayer((None, 3, 224, 224))\n",
    "    net['conv1_1'] = ConvLayer(net['input'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['conv1_2'] = ConvLayer(net['conv1_1'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['pool1'] = PoolLayer(net['conv1_2'], 2)\n",
    "    net['conv2_1'] = ConvLayer(net['pool1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['conv2_2'] = ConvLayer(net['conv2_1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['pool2'] = PoolLayer(net['conv2_2'], 2)\n",
    "    net['conv3_1'] = ConvLayer(net['pool2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_2'] = ConvLayer(net['conv3_1'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_3'] = ConvLayer(net['conv3_2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_4'] = ConvLayer(net['conv3_3'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['pool3'] = PoolLayer(net['conv3_4'], 2)\n",
    "    net['conv4_1'] = ConvLayer(net['pool3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_2'] = ConvLayer(net['conv4_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_3'] = ConvLayer(net['conv4_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_4'] = ConvLayer(net['conv4_3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool4'] = PoolLayer(net['conv4_4'], 2)\n",
    "    net['conv5_1'] = ConvLayer(net['pool4'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_2'] = ConvLayer(net['conv5_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_3'] = ConvLayer(net['conv5_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_4'] = ConvLayer(net['conv5_3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool5'] = PoolLayer(net['conv5_4'], 2)\n",
    "    net['fc6'] = DenseLayer(net['pool5'], num_units=4096)\n",
    "    net['fc6_dropout'] = DropoutLayer(net['fc6'], p=0.5)\n",
    "    net['fc7'] = DenseLayer(net['fc6_dropout'], num_units=4096)\n",
    "    net['fc7_dropout'] = DropoutLayer(net['fc7'], p=0.5)\n",
    "    net['fc8_not_used'] = DenseLayer(net['fc7_dropout'], num_units=1000, nonlinearity=None)\n",
    "    net['prob_not_used'] = NonlinearityLayer(net['fc8_not_used'], softmax)\n",
    "    \n",
    "    set_vgg_params(net)\n",
    "\n",
    "    # VAE\n",
    "    net['enc_vae'] = DenseLayer(net['fc7_dropout'], num_units=128, nonlinearity=leaky_rectify)\n",
    "    net['muq_vae'] = DenseLayer(net['enc_vae'], num_units=NUM_LATENT_Z, nonlinearity=None)     #mu(x)\n",
    "    net['logvarq_vae'] = DenseLayer(net['enc_vae'], num_units=NUM_LATENT_Z, nonlinearity=lambda x: T.clip(x,-10,10)) #logvar(x)\n",
    "    net['z_vae'] = SimpleSampleLayer(mean=net['muq_vae'], log_var=net['logvarq_vae']) # sample a latent representation z \\sim q(z|x) = N(mu(x),logvar(x))\n",
    "    net['in_z_vae'] = InputLayer(shape=(None, NUM_LATENT_Z))\n",
    "    #net['dec_vae'] = DenseLayer(net['in_z_vae'], num_units=128, nonlinearity=leaky_rectify)\n",
    "\n",
    "    # Vanilla network\n",
    "    net['fc8'] = DenseLayer(net['in_z_vae'], num_units=128)\n",
    "    net['fc8_dropout'] = DropoutLayer(net['fc8'], p=0.5)\n",
    "    net['fc9'] = DenseLayer(net['fc8_dropout'], num_units=4, nonlinearity=None)\n",
    "    net['prob'] = NonlinearityLayer(net['fc9'], softmax)\n",
    "\n",
    "    return net\n",
    "\n",
    "def set_vgg_params(net):\n",
    "    model = pickle.load(open('vgg19.pkl'))\n",
    "\n",
    "    # Remove the trainable argument from the layers that can potentially have it\n",
    "    for key, val in net.iteritems():\n",
    "        if not ('dropout' or 'pool' in key):\n",
    "            net[key].params[net[key].W].remove(\"trainable\")\n",
    "            net[key].params[net[key].b].remove(\"trainable\")\n",
    "\n",
    "    lasagne.layers.set_all_param_values(net['prob_not_used'], model['param values'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(TRAIN_FILE, 'r') as f:\n",
    "        x_train = pickle.load(f)\n",
    "    with open(VALIDATION_FILE, 'r') as f:\n",
    "        x_valid = pickle.load(f)\n",
    "    with open(TARGET_FILE, 'r') as f:\n",
    "        targets_train = pickle.load(f)\n",
    "    with open(TARGET_VALIDATION_FILE, 'r') as f:\n",
    "        targets_valid = pickle.load(f)\n",
    "    \n",
    "\n",
    "    x_train = np.asarray(x_train).squeeze(axis=1)\n",
    "    x_valid = np.asarray(x_valid).squeeze(axis=1)\n",
    "    targets_train = np.asarray(targets_train, dtype=np.float32)\n",
    "    targets_valid = np.asarray(targets_valid, dtype=np.float32)\n",
    "    \n",
    "    return x_train, x_valid, targets_train, targets_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create network\n",
    "net = create_network()\n",
    "\n",
    "# TODO : Call function to get the data\n",
    "x_train, x_valid, targets_train, targets_valid = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394, 3, 224, 224)\n",
      "(78, 3, 224, 224)\n",
      "(394, 4)\n",
      "(78, 4)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print x_valid.shape\n",
    "print targets_train.shape\n",
    "print targets_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode=self.mode,\n",
      "/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode=self.mode,\n",
      "/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/pool.py:266: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode=self.mode,\n"
     ]
    }
   ],
   "source": [
    "f_train, f_eval, f_z, f_sample, f_recon = VAEHelper.create_theano_functions(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 5060952064 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuAllocEmpty(Shape_i{0}.0, Shape_i{0}.0, Elemwise{Composite{((i0 + i1) - i2)}}.0, Elemwise{Composite{((i0 + i1) - i2)}}.0)\nToposort index: 139\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), (), ()]\nInputs strides: [(), (), (), ()]\nInputs values: [array(394), array(64), array(224), array(224)]\nInputs type_num: [7, 7, 7, 7]\nOutputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode=(1, 1), subsample=(1, 1), conv_mode='cross', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-f856271dacf8>\", line 1, in <module>\n    f_train, f_eval, f_z, f_sample, f_recon = VAEHelper.create_theano_functions(net)\n  File \"<ipython-input-3-86df75f251ec>\", line 17, in create_theano_functions\n    {net['input']: sym_x}, deterministic=True)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/helper.py\", line 191, in get_output\n    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/conv.py\", line 330, in get_output_for\n    conved = self.convolve(input, **kwargs)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/dnn.py\", line 380, in convolve\n    conv_mode=conv_mode\n\nDebugprint of the apply node: \nGpuAllocEmpty [id A] <CudaNdarrayType(float32, 4D)> ''   \n |Shape_i{0} [id B] <TensorType(int64, scalar)> ''   \n | |x [id C] <TensorType(float32, 4D)>\n |Shape_i{0} [id D] <TensorType(int64, scalar)> ''   \n | |W [id E] <CudaNdarrayType(float32, 4D)>\n |Elemwise{Composite{((i0 + i1) - i2)}} [id F] <TensorType(int64, scalar)> ''   \n | |TensorConstant{3} [id G] <TensorType(int64, scalar)>\n | |Shape_i{2} [id H] <TensorType(int64, scalar)> ''   \n | | |x [id C] <TensorType(float32, 4D)>\n | |Shape_i{2} [id I] <TensorType(int64, scalar)> ''   \n |   |W [id E] <CudaNdarrayType(float32, 4D)>\n |Elemwise{Composite{((i0 + i1) - i2)}} [id J] <TensorType(int64, scalar)> ''   \n   |TensorConstant{3} [id G] <TensorType(int64, scalar)>\n   |Shape_i{3} [id K] <TensorType(int64, scalar)> ''   \n   | |x [id C] <TensorType(float32, 4D)>\n   |Shape_i{3} [id L] <TensorType(int64, scalar)> ''   \n     |W [id E] <CudaNdarrayType(float32, 4D)>\n\nStorage map footprint:\n - W, Shared Input, Shape: (25088, 4096), ElemSize: 4 Byte(s), TotalSize: 411041792 Byte(s)\n - x, Input, Shape: (394, 3, 224, 224), ElemSize: 4 Byte(s), TotalSize: 237232128 Byte(s)\n - GpuContiguous.0, Shape: (394, 3, 224, 224), ElemSize: 4 Byte(s), TotalSize: 237232128 Byte(s)\n - W, Shared Input, Shape: (4096, 4096), ElemSize: 4 Byte(s), TotalSize: 67108864 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 4718592 Byte(s)\n - W, Shared Input, Shape: (256, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 2359296 Byte(s)\n - W, Shared Input, Shape: (256, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 2359296 Byte(s)\n - W, Shared Input, Shape: (256, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 2359296 Byte(s)\n - W, Shared Input, Shape: (4096, 128), ElemSize: 4 Byte(s), TotalSize: 2097152 Byte(s)\n - W, Shared Input, Shape: (256, 128, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1179648 Byte(s)\n - W, Shared Input, Shape: (128, 128, 3, 3), ElemSize: 4 Byte(s), TotalSize: 589824 Byte(s)\n - GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.0, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - W, Shared Input, Shape: (128, 64, 3, 3), ElemSize: 4 Byte(s), TotalSize: 294912 Byte(s)\n - W, Shared Input, Shape: (64, 64, 3, 3), ElemSize: 4 Byte(s), TotalSize: 147456 Byte(s)\n - b, Shared Input, Shape: (4096,), ElemSize: 4 Byte(s), TotalSize: 16384 Byte(s)\n - b, Shared Input, Shape: (4096,), ElemSize: 4 Byte(s), TotalSize: 16384 Byte(s)\n - GpuContiguous.0, Shape: (64, 3, 3, 3), ElemSize: 4 Byte(s), TotalSize: 6912 Byte(s)\n - W, Shared Input, Shape: (64, 3, 3, 3), ElemSize: 4 Byte(s), TotalSize: 6912 Byte(s)\n - GpuReshape{2}.0, Shape: (394, 2), ElemSize: 4 Byte(s), TotalSize: 3152 Byte(s)\n - W, Shared Input, Shape: (128, 4), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - W, Shared Input, Shape: (128, 2), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - W, Shared Input, Shape: (128, 2), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - W, Shared Input, Shape: (2, 128), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - GpuFromHost.0, Shape: (10, 4), ElemSize: 4 Byte(s), TotalSize: 160 Byte(s)\n - y, Input, Shape: (10, 4), ElemSize: 4 Byte(s), TotalSize: 160 Byte(s)\n - b, Shared Input, Shape: (4,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 2}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 0}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{4}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{((i0 + i1) - i2)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{6}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{12}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - b, Shared Input, Shape: (2,), ElemSize: 4 Byte(s), TotalSize: 8 Byte(s)\n - b, Shared Input, Shape: (2,), ElemSize: 4 Byte(s), TotalSize: 8 Byte(s)\n - Elemwise{Composite{((i0 + i1) - i2)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{3}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{10}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{9}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - CudaNdarrayConstant{[[ 0.505]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 10.]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[  9.99999997e-07]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.99999899]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{-0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[[-10.]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[-2.]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[ 6.28318548]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.495]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.5]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[[[[ 0.5]]]]}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 1.]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 1035220889.0 Byte(s) 0.964 GB\n TotalSize inputs: 797985385.0 Byte(s) 0.743 GB\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4fe46b4e641b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamples_processed\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mvalid_samples_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msamples_processed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mLL_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlogpx_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# or could mean the log_px_given_z, KL_qp in ll function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/icarus/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    887\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/icarus/anaconda2/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/icarus/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error allocating 5060952064 bytes of device memory (CNMEM_STATUS_OUT_OF_MEMORY).\nApply node that caused the error: GpuAllocEmpty(Shape_i{0}.0, Shape_i{0}.0, Elemwise{Composite{((i0 + i1) - i2)}}.0, Elemwise{Composite{((i0 + i1) - i2)}}.0)\nToposort index: 139\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), (), ()]\nInputs strides: [(), (), (), ()]\nInputs values: [array(394), array(64), array(224), array(224)]\nInputs type_num: [7, 7, 7, 7]\nOutputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode=(1, 1), subsample=(1, 1), conv_mode='cross', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]\n\nBacktrace when the node is created(use Theano flag traceback.limit=N to make it longer):\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-f856271dacf8>\", line 1, in <module>\n    f_train, f_eval, f_z, f_sample, f_recon = VAEHelper.create_theano_functions(net)\n  File \"<ipython-input-3-86df75f251ec>\", line 17, in create_theano_functions\n    {net['input']: sym_x}, deterministic=True)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/helper.py\", line 191, in get_output\n    all_outputs[layer] = layer.get_output_for(layer_inputs, **kwargs)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/conv.py\", line 330, in get_output_for\n    conved = self.convolve(input, **kwargs)\n  File \"/home/icarus/anaconda2/lib/python2.7/site-packages/lasagne/layers/dnn.py\", line 380, in convolve\n    conv_mode=conv_mode\n\nDebugprint of the apply node: \nGpuAllocEmpty [id A] <CudaNdarrayType(float32, 4D)> ''   \n |Shape_i{0} [id B] <TensorType(int64, scalar)> ''   \n | |x [id C] <TensorType(float32, 4D)>\n |Shape_i{0} [id D] <TensorType(int64, scalar)> ''   \n | |W [id E] <CudaNdarrayType(float32, 4D)>\n |Elemwise{Composite{((i0 + i1) - i2)}} [id F] <TensorType(int64, scalar)> ''   \n | |TensorConstant{3} [id G] <TensorType(int64, scalar)>\n | |Shape_i{2} [id H] <TensorType(int64, scalar)> ''   \n | | |x [id C] <TensorType(float32, 4D)>\n | |Shape_i{2} [id I] <TensorType(int64, scalar)> ''   \n |   |W [id E] <CudaNdarrayType(float32, 4D)>\n |Elemwise{Composite{((i0 + i1) - i2)}} [id J] <TensorType(int64, scalar)> ''   \n   |TensorConstant{3} [id G] <TensorType(int64, scalar)>\n   |Shape_i{3} [id K] <TensorType(int64, scalar)> ''   \n   | |x [id C] <TensorType(float32, 4D)>\n   |Shape_i{3} [id L] <TensorType(int64, scalar)> ''   \n     |W [id E] <CudaNdarrayType(float32, 4D)>\n\nStorage map footprint:\n - W, Shared Input, Shape: (25088, 4096), ElemSize: 4 Byte(s), TotalSize: 411041792 Byte(s)\n - x, Input, Shape: (394, 3, 224, 224), ElemSize: 4 Byte(s), TotalSize: 237232128 Byte(s)\n - GpuContiguous.0, Shape: (394, 3, 224, 224), ElemSize: 4 Byte(s), TotalSize: 237232128 Byte(s)\n - W, Shared Input, Shape: (4096, 4096), ElemSize: 4 Byte(s), TotalSize: 67108864 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 512, 3, 3), ElemSize: 4 Byte(s), TotalSize: 9437184 Byte(s)\n - W, Shared Input, Shape: (512, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 4718592 Byte(s)\n - W, Shared Input, Shape: (256, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 2359296 Byte(s)\n - W, Shared Input, Shape: (256, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 2359296 Byte(s)\n - W, Shared Input, Shape: (256, 256, 3, 3), ElemSize: 4 Byte(s), TotalSize: 2359296 Byte(s)\n - W, Shared Input, Shape: (4096, 128), ElemSize: 4 Byte(s), TotalSize: 2097152 Byte(s)\n - W, Shared Input, Shape: (256, 128, 3, 3), ElemSize: 4 Byte(s), TotalSize: 1179648 Byte(s)\n - W, Shared Input, Shape: (128, 128, 3, 3), ElemSize: 4 Byte(s), TotalSize: 589824 Byte(s)\n - GPU_mrg_uniform{CudaNdarrayType(float32, vector),inplace}.0, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - <CudaNdarrayType(float32, vector)>, Shared Input, Shape: (92160,), ElemSize: 4 Byte(s), TotalSize: 368640 Byte(s)\n - W, Shared Input, Shape: (128, 64, 3, 3), ElemSize: 4 Byte(s), TotalSize: 294912 Byte(s)\n - W, Shared Input, Shape: (64, 64, 3, 3), ElemSize: 4 Byte(s), TotalSize: 147456 Byte(s)\n - b, Shared Input, Shape: (4096,), ElemSize: 4 Byte(s), TotalSize: 16384 Byte(s)\n - b, Shared Input, Shape: (4096,), ElemSize: 4 Byte(s), TotalSize: 16384 Byte(s)\n - GpuContiguous.0, Shape: (64, 3, 3, 3), ElemSize: 4 Byte(s), TotalSize: 6912 Byte(s)\n - W, Shared Input, Shape: (64, 3, 3, 3), ElemSize: 4 Byte(s), TotalSize: 6912 Byte(s)\n - GpuReshape{2}.0, Shape: (394, 2), ElemSize: 4 Byte(s), TotalSize: 3152 Byte(s)\n - W, Shared Input, Shape: (128, 4), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (512,), ElemSize: 4 Byte(s), TotalSize: 2048 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (256,), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - W, Shared Input, Shape: (128, 2), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - W, Shared Input, Shape: (128, 2), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - W, Shared Input, Shape: (2, 128), ElemSize: 4 Byte(s), TotalSize: 1024 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (128,), ElemSize: 4 Byte(s), TotalSize: 512 Byte(s)\n - b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - b, Shared Input, Shape: (64,), ElemSize: 4 Byte(s), TotalSize: 256 Byte(s)\n - GpuFromHost.0, Shape: (10, 4), ElemSize: 4 Byte(s), TotalSize: 160 Byte(s)\n - y, Input, Shape: (10, 4), ElemSize: 4 Byte(s), TotalSize: 160 Byte(s)\n - b, Shared Input, Shape: (4,), ElemSize: 4 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 2}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{(2,) of 0}, Shape: (2,), ElemSize: 8 Byte(s), TotalSize: 16 Byte(s)\n - TensorConstant{4}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Elemwise{Composite{((i0 + i1) - i2)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{3}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{2}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{6}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{12}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - b, Shared Input, Shape: (2,), ElemSize: 4 Byte(s), TotalSize: 8 Byte(s)\n - b, Shared Input, Shape: (2,), ElemSize: 4 Byte(s), TotalSize: 8 Byte(s)\n - Elemwise{Composite{((i0 + i1) - i2)}}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{0}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - Shape_i{2}.0, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{-1}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{3}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{10}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - TensorConstant{9}, Shape: (), ElemSize: 8 Byte(s), TotalSize: 8.0 Byte(s)\n - CudaNdarrayConstant{[[ 0.505]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 10.]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[  9.99999997e-07]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.99999899]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{-0.5}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - Constant{1.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[[-10.]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[-2.]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[ 6.28318548]}, Shape: (1,), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.495]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 0.5]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - Constant{0.0}, Shape: (), ElemSize: 4 Byte(s), TotalSize: 4.0 Byte(s)\n - CudaNdarrayConstant{[[[[ 0.5]]]]}, Shape: (1, 1, 1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - CudaNdarrayConstant{[[ 1.]]}, Shape: (1, 1), ElemSize: 4 Byte(s), TotalSize: 4 Byte(s)\n - TensorConstant{0}, Shape: (), ElemSize: 1 Byte(s), TotalSize: 1.0 Byte(s)\n TotalSize: 1035220889.0 Byte(s) 0.964 GB\n TotalSize inputs: 797985385.0 Byte(s) 0.743 GB\n\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "samples_to_process = 1e4\n",
    "val_interval = 5e2\n",
    "\n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "plt.figure(figsize=(12, 24))\n",
    "valid_samples_processed = []\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        _LL_train, _KL_train, _logpx_train = [],[],[]\n",
    "        idxs = np.random.choice(range(x_train.shape[0]), size=(batch_size), replace=False)\n",
    "        x_batch = x_train[idxs]\n",
    "        y_batch = targets_train[idxs]\n",
    "#         print x_batch.shape\n",
    "        out = f_train(x_batch, y_batch)\n",
    "        samples_processed += batch_size\n",
    "\n",
    "        if samples_processed % val_interval == 0:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            out = f_eval(x_train, y_batch)\n",
    "            LL_train += [out[0]]\n",
    "            logpx_train += [out[1][:,0]] # or could mean the log_px_given_z, KL_qp in ll function\n",
    "            KL_train += [out[2][:,0]]\n",
    "\n",
    "            out = f_eval(x_valid)\n",
    "            LL_valid += [out[0]]\n",
    "            logpx_valid += [out[1][:,0]] #just pick a single sample, or could mean the log_px_given_z, KL_qp in ll function\n",
    "            KL_valid += [out[2][:,0]]\n",
    "\n",
    "            z_eval = f_z(x_valid)[0]\n",
    "            x_sample = f_sample(np.random.normal(size=(batch_size, NUM_LATENT_Z)).astype('float32'))[0]\n",
    "            x_recon = f_recon(x_valid)[0]\n",
    "\n",
    "            plt.subplot(NUM_CLASSES+2,2,1)\n",
    "            plt.legend(['LL', 'log(p(x))'], loc=2)\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, LL_train, color=\"black\")\n",
    "            plt.plot(valid_samples_processed, logpx_train, color=\"red\")\n",
    "            plt.plot(valid_samples_processed, LL_valid, color=\"black\", linestyle=\"--\")\n",
    "            plt.plot(valid_samples_processed, logpx_valid, color=\"red\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "\n",
    "            plt.subplot(NUM_CLASSES+2,2,2)\n",
    "            plt.cla()\n",
    "            plt.xlabel('z0'), plt.ylabel('z1')\n",
    "            color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, NUM_CLASSES)))\n",
    "            for i in range(NUM_CLASSES):\n",
    "                clr = next(color)\n",
    "                plt.scatter(z_eval[targets_valid==i, 0], z_eval[targets_valid==i, 1], c=clr, s=5., lw=0, marker='o', )\n",
    "            plt.grid('on')\n",
    "\n",
    "            plt.savefig(\"out52.png\")\n",
    "            display(Image(filename=\"out52.png\"))\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            plt.subplot(NUM_CLASSES+2,2,3)\n",
    "            plt.legend(['KL(q||p)'])\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, KL_train, color=\"blue\")\n",
    "            plt.plot(valid_samples_processed, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "\n",
    "            plt.subplot(NUM_CLASSES+2,2,4)\n",
    "            plt.cla()\n",
    "            plt.title('Samples')\n",
    "            plt.axis('off')\n",
    "            idx = 0\n",
    "            canvas = np.zeros((28*10, 10*28))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_sample[idx].reshape((28, 28))\n",
    "                    idx += 1\n",
    "            plt.imshow(canvas, cmap='gray')\n",
    "\n",
    "            c=0\n",
    "            for k in range(5, 5 + NUM_CLASSES*2, 2):\n",
    "                plt.subplot(NUM_CLASSES+2,2,k)\n",
    "                plt.cla()\n",
    "                plt.title('Inputs for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_valid[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "\n",
    "                plt.subplot(NUM_CLASSES+2,2,k+1)\n",
    "                plt.cla()\n",
    "                plt.title('Reconstructions for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_recon[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "                c += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

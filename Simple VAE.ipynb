{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icarus/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean value', 'synset words', 'model name', 'param values']\n",
      "[ 103.939  116.779  123.68 ]\n",
      "Loaded VGG!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import math\n",
    "import os\n",
    "import skimage.io as io\n",
    "from pycocotools.coco import COCO\n",
    "from skimage.io import imread\n",
    "import skimage.transform\n",
    "import pickle\n",
    "\n",
    "from imagenet import VGGLoader\n",
    "vgg = VGGLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSCOCO\n",
    "First let's load the MSCOCO dataset and plot a few examples. We only load a limited amount of number classes, so that we can speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndata = np.load(\\'../week1/mnist.npz\\')\\nnum_classes = 5\\nidxs_train = []\\nidxs_valid = []\\nidxs_test = []\\nfor i in range(num_classes):\\n    idxs_train += np.where(data[\\'y_train\\'] == i)[0].tolist()\\n    idxs_valid += np.where(data[\\'y_valid\\'] == i)[0].tolist()\\n    idxs_test += np.where(data[\\'y_test\\'] == i)[0].tolist()\\n\\nx_train = bernoullisample(data[\\'X_train\\'][idxs_train]).astype(\\'float32\\')\\ntargets_train = data[\\'y_train\\'][idxs_train].astype(\\'int32\\') # Since this is unsupervised, the targets are only used for validation.\\nx_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\\n\\nx_valid = bernoullisample(data[\\'X_valid\\'][idxs_valid]).astype(\\'float32\\')\\ntargets_valid = data[\\'y_valid\\'][idxs_valid].astype(\\'int32\\')\\n\\nx_test = bernoullisample(data[\\'X_test\\'][idxs_test]).astype(\\'float32\\')\\ntargets_test = data[\\'y_test\\'][idxs_test].astype(\\'int32\\')\\n\\nprint(\"training set dim(%i, %i).\" % x_train.shape)\\nprint(\"validation set dim(%i, %i).\" % x_valid.shape)\\nprint(\"test set dim(%i, %i).\" % x_test.shape)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import matplotlib.image as mpimg\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "#To speed up training we'll only work on a subset of the data\n",
    "#We discretize the data to 0 and 1 in order to use it with a bernoulli observation model p(x|z) = Ber(mu(z))\n",
    "\n",
    "#def bernoullisample(x):\n",
    "#    return np.random.binomial(1,x,size=x.shape).astype(theano.config.floatX)\n",
    "\n",
    "imgDict = pickle.load( open( \"ImgDictAnno1000SamplesValidation.p\", \"rb\" ) )\n",
    "x_train = []\n",
    "targets_train = []\n",
    "fig = 0\n",
    "counter = 0\n",
    "for image in imgDict:\n",
    "    try:\n",
    "        raw, processed = vgg.prep_image(image[2:])\n",
    "        x_train.append(processed)\n",
    "        targets_train.append(imgDict[image])\n",
    "    except:\n",
    "        counter += 1\n",
    "\n",
    "print(counter)\n",
    "\n",
    "'''\n",
    "plt.figure(fig)\n",
    "plt.imshow(mpimg.imread(image))\n",
    "plt.show()\n",
    "fig += 1\n",
    "'''\n",
    "\n",
    "    \n",
    "'''\n",
    "data = np.load('../week1/mnist.npz')\n",
    "num_classes = 5\n",
    "idxs_train = []\n",
    "idxs_valid = []\n",
    "idxs_test = []\n",
    "for i in range(num_classes):\n",
    "    idxs_train += np.where(data['y_train'] == i)[0].tolist()\n",
    "    idxs_valid += np.where(data['y_valid'] == i)[0].tolist()\n",
    "    idxs_test += np.where(data['y_test'] == i)[0].tolist()\n",
    "\n",
    "x_train = bernoullisample(data['X_train'][idxs_train]).astype('float32')\n",
    "targets_train = data['y_train'][idxs_train].astype('int32') # Since this is unsupervised, the targets are only used for validation.\n",
    "x_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\n",
    "\n",
    "x_valid = bernoullisample(data['X_valid'][idxs_valid]).astype('float32')\n",
    "targets_valid = data['y_valid'][idxs_valid].astype('int32')\n",
    "\n",
    "x_test = bernoullisample(data['X_test'][idxs_test]).astype('float32')\n",
    "targets_test = data['y_test'][idxs_test].astype('int32')\n",
    "\n",
    "print(\"training set dim(%i, %i).\" % x_train.shape)\n",
    "print(\"validation set dim(%i, %i).\" % x_valid.shape)\n",
    "print(\"test set dim(%i, %i).\" % x_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump( x_train, open( \"x_train_1000SamplesValidation.p\", \"wb\" ) )\n",
    "pickle.dump( targets_train, open( \"targets_train_1000SamplesValidation.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and display image and use url to load image\n",
    "I = io.imread('')\n",
    "plt.figure() \n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the lower bound, we define following density functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defined a couple of helper functions\n",
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def log_bernoulli(x, p, eps=0.0):\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "def kl_normal2_stdnormal(mean, log_var):\n",
    "    return -0.5*(1 + log_var - mean**2 - T.exp(log_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,get_output, get_all_params\n",
    "from lasagne.nonlinearities import rectify, sigmoid, leaky_rectify\n",
    "from samplelayer import SimpleSampleLayer\n",
    "\n",
    "num_features = x_train.shape[-1]\n",
    "num_latent_z = 20\n",
    "\n",
    "#MODEL SPECIFICATION\n",
    "\n",
    "#ENCODER\n",
    "l_in_x = vgg.net['fc7_dropout']\n",
    "l_enc = DenseLayer(l_in_x, num_units=128, nonlinearity=leaky_rectify)\n",
    "#l_enc_2 = DenseLayer(l_enc, num_units=64, nonlinearity=leaky_rectify)\n",
    "l_muq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=None)     #mu(x)\n",
    "l_logvarq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=lambda x: T.clip(x,-10,10)) #logvar(x), \n",
    "l_z = SimpleSampleLayer(mean=l_muq, log_var=l_logvarq) #sample a latent representation z \\sim q(z|x) = N(mu(x),logvar(x))\n",
    "#we split the model into two parts to allow sampling from the decoder model separately\n",
    "#DECODER\n",
    "l_in_z = InputLayer(shape=(None, num_latent_z))\n",
    "l_dec = DenseLayer(l_in_z, num_units=128, nonlinearity=leaky_rectify) \n",
    "#l_dec_2 = DenseLayer(l_dec, num_units=64, nonlinearity=leaky_rectify) \n",
    "l_mux = DenseLayer(l_dec, num_units=num_features, nonlinearity=sigmoid)  #reconstruction of input using a sigmoid output since mux \\in [0,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the Theano functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sym_x = T.matrix('x')\n",
    "sym_z = T.matrix('z')\n",
    "\n",
    "z_train, muq_train, logvarq_train = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x},deterministic=False)\n",
    "mux_train = get_output(l_mux,{l_in_z:z_train},deterministic=False)\n",
    "\n",
    "z_eval, muq_eval, logvarq_eval = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x},deterministic=True)\n",
    "mux_eval = get_output(l_mux,{l_in_z:z_eval},deterministic=True)\n",
    "\n",
    "mux_sample = get_output(l_mux,{l_in_z:sym_z},deterministic=True)\n",
    "\n",
    "#define the cost function\n",
    "def LogLikelihood(mux,x,muq,logvarq):\n",
    "    log_px_given_z = log_bernoulli(x, mux, eps=1e-6).sum(axis=1).mean() #note that we sum the latent dimension and mean over the samples\n",
    "    KL_qp = kl_normal2_stdnormal(muq, logvarq).sum(axis=1).mean() # * 0 # To ignore the KL term\n",
    "    LL = log_px_given_z - KL_qp\n",
    "    return LL, log_px_given_z, KL_qp\n",
    "\n",
    "LL_train, logpx_train, KL_train = LogLikelihood(mux_train, sym_x, muq_train, logvarq_train)\n",
    "LL_eval, logpx_eval, KL_eval = LogLikelihood(mux_eval, sym_x, muq_eval, logvarq_eval)\n",
    "\n",
    "all_params = get_all_params([l_z,l_mux],trainable=True)\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(-LL_train, all_params)\n",
    "\n",
    "# Set the update function for parameters. The Adam optimizer works really well with VAEs.\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=1e-2)\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x],\n",
    "                          outputs=[LL_train, logpx_train, KL_train],\n",
    "                          updates=updates)\n",
    "\n",
    "f_eval = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_eval, logpx_eval, KL_eval])\n",
    "\n",
    "f_z = theano.function(inputs=[sym_x],\n",
    "                         outputs=[z_eval])\n",
    "\n",
    "f_sample = theano.function(inputs=[sym_z],\n",
    "                         outputs=[mux_sample])\n",
    "\n",
    "f_recon = theano.function(inputs=[sym_x],\n",
    "                         outputs=[mux_eval])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we sample each batch and evaluate the error, latent space and reconstructions every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "samples_to_process = 1e4\n",
    "val_interval = 5e2\n",
    " \n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "plt.figure(figsize=(12, 24))\n",
    "valid_samples_processed = []\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        _LL_train, _KL_train, _logpx_train = [],[],[]\n",
    "        idxs = np.random.choice(range(x_train.shape[0]), size=(batch_size), replace=False)  \n",
    "        x_batch = x_train[idxs]\n",
    "        out = f_train(x_batch)\n",
    "        samples_processed += batch_size\n",
    "           \n",
    "        if samples_processed % val_interval == 0:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            out = f_eval(x_train)\n",
    "            LL_train += [out[0]] \n",
    "            logpx_train += [out[1]]\n",
    "            KL_train += [out[2]]\n",
    "            \n",
    "            out = f_eval(x_valid)\n",
    "            LL_valid += [out[0]]\n",
    "            logpx_valid += [out[1]]\n",
    "            KL_valid += [out[2]]\n",
    "            \n",
    "            z_eval = f_z(x_valid)[0]\n",
    "            x_sample = f_sample(np.random.normal(size=(100, num_latent_z)).astype('float32'))[0]\n",
    "            x_recon = f_recon(x_valid)[0]\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,1)\n",
    "            plt.legend(['LL', 'log(p(x))'], loc=2)\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, LL_train, color=\"black\")\n",
    "            plt.plot(valid_samples_processed, logpx_train, color=\"red\")\n",
    "            plt.plot(valid_samples_processed, LL_valid, color=\"black\", linestyle=\"--\")\n",
    "            plt.plot(valid_samples_processed, logpx_valid, color=\"red\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,2)\n",
    "            plt.cla()\n",
    "            plt.xlabel('z0'), plt.ylabel('z1')\n",
    "            color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "            for i in range(num_classes):\n",
    "                clr = next(color)\n",
    "                plt.scatter(z_eval[targets_valid==i, 0], z_eval[targets_valid==i, 1], c=clr, s=5., lw=0, marker='o', )\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.savefig(\"out52.png\")\n",
    "            display(Image(filename=\"out52.png\"))\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,3)\n",
    "            plt.legend(['KL(q||p)'])\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, KL_train, color=\"blue\")\n",
    "            plt.plot(valid_samples_processed, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,4)\n",
    "            plt.cla()\n",
    "            plt.title('Samples')\n",
    "            plt.axis('off')\n",
    "            idx = 0\n",
    "            canvas = np.zeros((28*10, 10*28))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_sample[idx].reshape((28, 28))\n",
    "                    idx += 1\n",
    "            plt.imshow(canvas, cmap='gray')\n",
    "            \n",
    "            c=0\n",
    "            for k in range(5, 5 + num_classes*2, 2):\n",
    "                plt.subplot(num_classes+2,2,k)\n",
    "                plt.cla()\n",
    "                plt.title('Inputs for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_valid[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "\n",
    "                plt.subplot(num_classes+2,2,k+1)\n",
    "                plt.cla()\n",
    "                plt.title('Reconstructions for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_recon[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "                c += 1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
